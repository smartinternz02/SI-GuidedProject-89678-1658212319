{"cells": [{"metadata": {}, "id": "5c6b22a9", "cell_type": "code", "source": "from tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)", "execution_count": 1, "outputs": []}, {"metadata": {}, "id": "32e2383b", "cell_type": "code", "source": "import tensorflow as tf\nprint(tf.__version__)", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "2.7.2\n", "name": "stdout"}]}, {"metadata": {}, "id": "2e0fbc5b", "cell_type": "code", "source": "# import the libraries as shown below\n\nfrom tensorflow.keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\nfrom glob import glob\n#import matplotlib.pyplot as plt", "execution_count": 3, "outputs": []}, {"metadata": {}, "id": "379caa10", "cell_type": "code", "source": "# re-size all the images to this\nIMAGE_SIZE = [224, 224]\n\ntrain_path = 'C:/Users/priya/Downloads/sp/dataset/train/'\nvalid_path = 'C:/Users/priya/Downloads/sp/dataset/test/'", "execution_count": 4, "outputs": []}, {"metadata": {}, "id": "c2ce4f3d", "cell_type": "code", "source": "# Import the VGG16 library as shown below and add preprocessing layer to the front of VGG\n# Here we will be using imagenet weights\n\nvgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 1s 0us/step\n58900480/58889256 [==============================] - 1s 0us/step\n", "name": "stdout"}]}, {"metadata": {}, "id": "88f8b974", "cell_type": "code", "source": "# don't train existing weights\nfor layer in vgg16.layers:\n    layer.trainable = False", "execution_count": 6, "outputs": []}, {"metadata": {}, "id": "2a8b02f5", "cell_type": "code", "source": "folders = glob('C:/Users/priya/Downloads/sp/dataset/test/*')", "execution_count": 7, "outputs": []}, {"metadata": {}, "id": "857b8116", "cell_type": "code", "source": "folders", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "[]"}, "metadata": {}}]}, {"metadata": {}, "id": "7883cd0b", "cell_type": "code", "source": "# our layers - you can add more if you want\nx = Flatten()(vgg16.output)", "execution_count": 9, "outputs": []}, {"metadata": {}, "id": "2fabb8c3", "cell_type": "code", "source": "len(folders)", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "0"}, "metadata": {}}]}, {"metadata": {}, "id": "04dda579", "cell_type": "code", "source": "prediction = Dense(17, activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=vgg16.input, outputs=prediction)", "execution_count": 11, "outputs": []}, {"metadata": {}, "id": "6f19de7a", "cell_type": "code", "source": "# view the structure of the model\nmodel.summary()", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n dense (Dense)               (None, 17)                426513    \n                                                                 \n=================================================================\nTotal params: 15,141,201\nTrainable params: 426,513\nNon-trainable params: 14,714,688\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "id": "476de74c", "cell_type": "code", "source": "# tell the model what cost and optimization method to use\nmodel.compile(\n  loss='categorical_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy']\n)", "execution_count": 13, "outputs": []}, {"metadata": {}, "id": "0b7f20fc", "cell_type": "code", "source": "# Use the Image Data Generator to import the images from the dataset\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen =ImageDataGenerator(rescale = 1./255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1./255)", "execution_count": 14, "outputs": []}, {"metadata": {}, "id": "0645f0f2", "cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_cc72ba4862694b39beee7a14fb77f07a = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='XqVhWztQrEwwSEE5mIO-7kLenBRUmJHoIO6z41MfcVp6',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n\nstreaming_body_1 = client_cc72ba4862694b39beee7a14fb77f07a.get_object(Bucket='humannailimageprocessing-donotdelete-pr-vowofyavnadgnv', Key='dataset1.zip')['Body']\n\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\n", "execution_count": 15, "outputs": []}, {"metadata": {}, "id": "b3e1f800", "cell_type": "code", "source": "from io import BytesIO\nimport zipfile\nunzip = zipfile.ZipFile(BytesIO(streaming_body_1.read()),'r')\nfile_paths = unzip.namelist()\nfor path in file_paths:\n    unzip.extract(path)", "execution_count": 16, "outputs": []}, {"metadata": {}, "id": "b39d06b7", "cell_type": "code", "source": "pwd", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "'/home/wsuser/work'"}, "metadata": {}}]}, {"metadata": {}, "id": "53872a6d", "cell_type": "code", "source": "# Make sure you provide the same target size as initialied for the image size\ntraining_set = train_datagen.flow_from_directory('/home/wsuser/work/dataset/train/',\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Found 647 images belonging to 17 classes.\n", "name": "stdout"}]}, {"metadata": {}, "id": "4f4dbb2f", "cell_type": "code", "source": "test_set = test_datagen.flow_from_directory('/home/wsuser/work/dataset/test/',\n                                            target_size = (224, 224),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "Found 183 images belonging to 17 classes.\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "id": "800c430e", "cell_type": "code", "source": "# fit the model\nr = model.fit_generator(\n  training_set,\n  validation_data=test_set,\n  epochs=20,\n  steps_per_epoch=len(training_set),\n  validation_steps=len(test_set)\n)", "execution_count": null, "outputs": [{"output_type": "stream", "text": "/tmp/wsuser/ipykernel_164/1716041270.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  r = model.fit_generator(\n", "name": "stderr"}, {"output_type": "stream", "text": "Epoch 1/20\n21/21 [==============================] - 300s 14s/step - loss: 3.4641 - accuracy: 0.1144 - val_loss: 2.7093 - val_accuracy: 0.1967\nEpoch 2/20\n21/21 [==============================] - 299s 15s/step - loss: 2.5389 - accuracy: 0.2519 - val_loss: 1.9979 - val_accuracy: 0.3443\nEpoch 3/20\n21/21 [==============================] - 298s 14s/step - loss: 2.0890 - accuracy: 0.3478 - val_loss: 1.8191 - val_accuracy: 0.4208\nEpoch 4/20\n21/21 [==============================] - 298s 14s/step - loss: 1.9136 - accuracy: 0.4374 - val_loss: 1.8592 - val_accuracy: 0.5191\nEpoch 5/20\n21/21 [==============================] - 299s 14s/step - loss: 1.7249 - accuracy: 0.4977 - val_loss: 1.6969 - val_accuracy: 0.5137\nEpoch 6/20\n21/21 [==============================] - 300s 14s/step - loss: 1.5516 - accuracy: 0.5379 - val_loss: 1.4689 - val_accuracy: 0.5847\nEpoch 7/20\n21/21 [==============================] - 298s 14s/step - loss: 1.4195 - accuracy: 0.5966 - val_loss: 1.2556 - val_accuracy: 0.6011\nEpoch 8/20\n21/21 [==============================] - 301s 14s/step - loss: 1.2693 - accuracy: 0.6136 - val_loss: 1.1603 - val_accuracy: 0.6776\nEpoch 9/20\n21/21 [==============================] - 300s 14s/step - loss: 1.2505 - accuracy: 0.6306 - val_loss: 1.1214 - val_accuracy: 0.6721\nEpoch 10/20\n21/21 [==============================] - 299s 14s/step - loss: 1.1606 - accuracy: 0.6646 - val_loss: 1.0734 - val_accuracy: 0.6940\nEpoch 11/20\n21/21 [==============================] - 300s 14s/step - loss: 1.0310 - accuracy: 0.7187 - val_loss: 0.8930 - val_accuracy: 0.7541\nEpoch 12/20\n21/21 [==============================] - 300s 14s/step - loss: 0.9730 - accuracy: 0.7558 - val_loss: 0.8861 - val_accuracy: 0.7814\nEpoch 13/20\n21/21 [==============================] - 300s 14s/step - loss: 0.8937 - accuracy: 0.7589 - val_loss: 0.8284 - val_accuracy: 0.7923\nEpoch 14/20\n21/21 [==============================] - 299s 14s/step - loss: 0.8179 - accuracy: 0.7913 - val_loss: 0.7392 - val_accuracy: 0.8361\nEpoch 15/20\n21/21 [==============================] - 298s 14s/step - loss: 0.8059 - accuracy: 0.7929 - val_loss: 0.7414 - val_accuracy: 0.8142\nEpoch 16/20\n21/21 [==============================] - 298s 15s/step - loss: 0.8821 - accuracy: 0.7481 - val_loss: 0.8279 - val_accuracy: 0.8306\nEpoch 17/20\n21/21 [==============================] - 298s 14s/step - loss: 0.8026 - accuracy: 0.7836 - val_loss: 0.6379 - val_accuracy: 0.8525\nEpoch 18/20\n21/21 [==============================] - 297s 14s/step - loss: 0.7471 - accuracy: 0.8145 - val_loss: 0.8601 - val_accuracy: 0.6940\nEpoch 19/20\n16/21 [=====================>........] - ETA: 54s - loss: 0.7484 - accuracy: 0.7988 ", "name": "stdout"}]}, {"metadata": {}, "id": "2928aee1", "cell_type": "code", "source": "import matplotlib.pyplot as plt", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "83849825", "cell_type": "code", "source": "# plot the loss\nplt.plot(r.history['loss'], label='train loss')\nplt.plot(r.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\nplt.savefig('LossVal_loss')\n\n# plot the accuracy\nplt.plot(r.history['accuracy'], label='train acc')\nplt.plot(r.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "962838b3", "cell_type": "code", "source": "# save it as a h5 file\n\n\nfrom tensorflow.keras.models import load_model\n\nmodel.save('model_vgg16.h5')", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "b1d664bd", "cell_type": "code", "source": "!tar -zcvf human-nail-image-processing-model_new.tgz model_vgg16.h5", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "c345942f", "cell_type": "code", "source": "ls -1", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d566cc9d", "cell_type": "code", "source": "!pip install watson-machine-learning-client --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "9584d8f0", "cell_type": "code", "source": "# Replace the credentials that you got from Watson Machine Learning service\nfrom ibm_watson_machine_learning import APIClient\nwml_credentials = { \n                    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n                    \"apikey\":\"sw5lgpdTNhaB-Sa1zX8-Hsarwmnj3WP9huKS8sBQS5Ai\"\n                  }\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "050c98c3", "cell_type": "code", "source": "client = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "28ba7b89", "cell_type": "code", "source": "def guid_from_space_name(client, space_name):\n    space = client.spaces.get_details()\n    #print(space)\n    return(next(item for item in space['resources'] if item['entity'][\"name\"] == space_name)['metadata']['id'])", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "cc6f2ca7", "cell_type": "code", "source": "space_uid = guid_from_space_name(client, 'humannailimageprocessing')\nprint(\"Space UID = \" + space_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "dba277b0", "cell_type": "code", "source": "client.set.default_space(space_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "5c8b8857", "cell_type": "code", "source": "client.software_specifications.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "ef92a70d", "cell_type": "code", "source": "software_spec_uid = client.software_specifications.get_uid_by_name(\"tensorflow_rt22.1-py3.9\")\nsoftware_spec_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "63bbcfb1", "cell_type": "code", "source": "model_details=client.repository.store_model(model='human-nail-image-processing-model_new.tgz',meta_props={\n    client.repository.ModelMetaNames.NAME:\"CNN\",\n    client.repository.ModelMetaNames.TYPE:\"tensorflow_2.7\",\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID:software_spec_uid}\n                                           )\nmodel_id = client.repository.get_model_uid(model_details)\n                    ", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a7f6975e", "cell_type": "code", "source": "model_id", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "04c4d345", "cell_type": "code", "source": "client.repository.download(model_id,'my_model.tar.gz')", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2b26e548", "cell_type": "code", "source": "y_pred = model.predict(test_set)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "26743ae9", "cell_type": "code", "source": "y_pred", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2ffb8307", "cell_type": "code", "source": "import numpy as np\ny_pred = np.argmax(y_pred, axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a32a35e2", "cell_type": "code", "source": "y_pred", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "613072d6", "cell_type": "code", "source": "from tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.mobilenet import preprocess_input", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "f744baf2", "cell_type": "code", "source": "model=load_model('model_vgg16.h5')", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "5c251e6e", "cell_type": "code", "source": "img_data='C:/Users/priya/Downloads/image1.jpg'", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "65dbaf13", "cell_type": "code", "source": "img_data", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "9c67fda1", "cell_type": "code", "source": "\nstreaming_body_3 = client_cc72ba4862694b39beee7a14fb77f07a.get_object(Bucket='humannailimageprocessing-donotdelete-pr-vowofyavnadgnv', Key='image1.jpg')['Body']\n\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\n", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "cd03c544", "cell_type": "code", "source": "x.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "bcd70b92", "cell_type": "code", "source": "x=x/255", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d55eac87", "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nx=np.expand_dims(x,axis=0)\nimg_data=preprocess_input(x)\nimg_data.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0abe7890", "cell_type": "code", "source": "model.predict(img_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "4d56e2ec", "cell_type": "code", "source": "a=np.argmax(model.predict(img_data), axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "93d329db", "cell_type": "code", "source": "a==1", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "550c3991", "cell_type": "code", "source": "import tensorflow as tf", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "8cfb3d69", "cell_type": "code", "source": "tf.__version__", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "c362e925", "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}